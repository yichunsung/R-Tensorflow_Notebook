{
    "collab_server" : "",
    "contents" : "# Convoluation neural network demo\n############################\n#      Input               #\n#       ↓                  #\n# Convolution layer 1      #    \n#       ↓                  #\n# Max pooling layer 1      #\n#       ↓                  #\n# Convolution layer 2      #\n#       ↓                  #\n# Max pooling layer 2      #\n#       ↓                  #\n# Flatten layer            #\n#       ↓                  #\n# Fully connected layer 1  #\n#       ↓                  #\n# Fully connected layer 2  #\n#       ↓                  #\n#     Output               #\n############################\n# Library Tensorflow and other packages\nlibrary(tensorflow)\nlibrary(magrittr)\n\n# Define function for filter\n# [filter_height, filter_width, in_channels, out_channels]\nadd_conv_filter <- function(filterShape){ # random filter as Variable\n  filterForConvLayer <- tf$truncated_normal(filterShape, stddev = 0.1) %>% tf$Variable() \n  return(filterForConvLayer)\n}\n# Define function for bias\nadd_bias <- function(BiasShape){\n  bias <- tf$constant(0.1, shape = BiasShape) %>% tf$Variable()\n  return(bias)\n}\n# Define function for convolution layer\nadd_convolutionLayer <- function(inputData, filter_weight, activation_function = \"None\"){\n  conv2dLayer <- tf$nn$conv2d(input = inputData, # input data\n                              # filter should be shape(filter_height, filter_width, in_channels, out_channels)\n                              filter = filter_weight, \n                              strides = shape(1L, 2L, 2L, 1L), # strides = [1, x_strides, y_strides, 1]\n                              padding = 'SAME'\n                              )\n  if(activation_function == \"None\"){\n    output_result <- conv2dLayer\n  }else{\n    output_result <- activation_function(conv2dLayer)\n  }\n  return(output_result)\n}\n# Define function for Max pooling layer\nadd_maxpoolingLayer <- function(inputData){\n  MaxPooling <- tf$nn$max_pool(inputData, # input data should be [batch, height, width, channels]\n                               ksize = shape(1L, 2L, 2L, 1L), # 2*2 pixels for max pooling\n                               strides = shape(1L, 2L, 2L, 1L), # strides =  [1, x_strides, y_strides, 1]\n                               padding = 'SAME')\n  return(MaxPooling)\n}\n# Define function for flatten layer\nadd_flattenLayer <- function(inputData, numberOfFactors){\n  flatten_layer <- tf$reshape(inputData, shape(-1, numberOfFactors))\n  return(flatten_layer)\n}\n# Define function for fully connected layer\nadd_fullyConnectedLayer <- function(inputData, Weight_FCLayer, bias_FCLayer, activation_function = \"None\"){\n  Wx_plus_b <- tf$matmul(inputData, Weight_FCLayer)+bias_FCLayer\n  if(activation_function == \"None\"){\n    FC_output_result <- Wx_plus_b\n  }else{\n    FC_output_result <- activation_function(Wx_plus_b)\n  }\n  return(FC_output_result)\n}\n\n# Define compute_accuracy function\ncompute_accuracy <- function(model_result, v_xs, v_ys){\n  y_pre <- sess$run(model_result, feed_dict = dict(xs = v_xs))\n  correct_prediction <- tf$equal(tf$argmax(y_pre, 1L), tf$argmax(v_ys, 1L))\n  accuracy <- tf$cast(correct_prediction, tf$float32) %>% tf$reduce_mean(.)\n  result <- sess$run(accuracy, feed_dict = dict(xs = v_xs, ys = v_ys))\n  return(result)\n}\n\n# Model building =======\n## Setting placeholder\nxs <- tf$placeholder(tf$float32, shape(NULL, 784L)) # input data = 28*28(784 factors) pixels image.\nys <- tf$placeholder(tf$float32, shape(NULL, 10L)) # output = 10 labels (0~9)\nx_image <- tf$reshape(xs, shape(-1L, 28L, 28L, 1L)) # [batch, height, width, channels]\n## Convolution layer 1 \nconvolayer1 <- add_convolutionLayer(\n  inputData = x_image,\n  filter_weight = shape(5L, 5L, 1L, 32L) %>% add_conv_filter(),\n  activation_function = tf$nn$relu\n)\n## Max pooling layer 1\nmaxPooling_1 <- add_maxpoolingLayer(\n  convolayer1\n)\n## Convolution layer 2 \nconvolayer2 <- add_convolutionLayer(\n  inputData = maxPooling_1, \n  filter_weight = shape(4L, 4L, 32L, 64L) %>% add_conv_filter(),\n  activation_function = tf$nn$relu\n) \n## Max pooling layer 2 \nmaxPooling_2 <- add_maxpoolingLayer(\n  inputData = convolayer2\n)\n## Flatten layer\nflatLayer_output <- add_flattenLayer(\n  inputData = maxPooling_2,\n  numberOfFactors = c(2L*2L*64L) %>% as.numeric()\n)\n## Fully connected layer 1\nfcLayer_1 <- add_fullyConnectedLayer(\n  inputData = flatLayer_output,\n  Weight_FCLayer = shape(2L*2L*64L, 1024L) %>% tf$random_normal(., stddev = 0.1) %>% tf$Variable(), # Set first layer ouput = 1024\n  bias_FCLayer = shape(1024L) %>% add_bias(),\n  activation_function = tf$nn$relu\n)\n## Fully connected layer 2\noutput_result <- add_fullyConnectedLayer(\n  inputData = fcLayer_1,\n  Weight_FCLayer = shape(1024L, 10L) %>% tf$random_normal(., stddev = 0.1) %>% tf$Variable(), # Set output layer ouput = 10 labels\n  bias_FCLayer = shape(10L) %>% add_bias(),\n  activation_function = tf$nn$softmax\n)\n\n# Model training =======\n## Loss function (cross entropy)\ncross_entropy <- tf$reduce_mean(-tf$reduce_sum(ys*tf$log(output_result), \n                                               reduction_indices = 1L))\n## Gradient Descent and learning rate setting\nlearning_rate <- 0.001 # Set learning rate = 0.1\ntrain_step_by_GD <- tf$train$AdamOptimizer(learning_rate)$minimize(cross_entropy)\n\n## Session setting\nsess <- tf$Session()\ninit <- tf$global_variables_initializer()\nsess$run(init)\n\n# MNIST data loadding \ndatasets <- tf$contrib$learn$datasets\nmnist <- datasets$mnist$read_data_sets(\"MNIST-data\", one_hot = TRUE)\n# Running\nfor (i in 1:1000){\n  batches <- mnist$train$next_batch(100L)\n  batch_xs <- batches[[1]]\n  batch_ys <- batches[[2]]\n  sess$run(train_step_by_GD, feed_dict = dict(xs = batch_xs, ys = batch_ys))\n  if(i %% 50 == 0){\n    print(compute_accuracy(output_result, mnist$test$images, mnist$test$labels))\n   # sess$run(cross_entropy, feed_dict = dict(xs = batch_xs, ys = batch_ys)) %>% print()\n  }\n}\n",
    "created" : 1509288237398.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3364327967",
    "id" : "B9B544C6",
    "lastKnownWriteTime" : 1509351893,
    "last_content_update" : 1509351893191,
    "path" : "D:/Github_rep/R-Tensorflow_Notebook/CNN/cnn_demo.R",
    "project_path" : "CNN/cnn_demo.R",
    "properties" : {
        "docOutlineVisible" : "0",
        "source_window_id" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}